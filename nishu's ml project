{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1iATd0CcMcmi8x3yCEk4iUq2d7kSa0VFP","timestamp":1719294745012},{"file_id":"10B2s5af8INkHgIz_7XBjW3iNDbGY7ltx","timestamp":1718183746249}],"collapsed_sections":["3RnN4peoiCZX","x71ZqKXriCWQ","7hBIi_osiCS2","JlHwYmJAmNHm","35m5QtbWiB9F","PoPl-ycgm1ru","H0kj-8xxnORC","nA9Y7ga8ng1Z","PBTbrJXOngz2","u3PMJOP6ngxN","dauF4eBmngu3","bKJF3rekwFvQ","MSa1f5Uengrz","GF8Ens_Soomf","K5QZ13OEpz2H","lQ7QKXXCp7Bj","448CDAPjqfQr","KSlN3yHqYklG","t6dVpIINYklI","ijmpgYnKYklI","-JiQyfWJYklI","EM7whBJCYoAo","fge-S5ZAYoAp","85gYPyotYoAp","RoGjAbkUYoAp","4Of9eVA-YrdM","iky9q4vBYrdO","F6T5p64dYrdO","y-Ehk30pYrdP","bamQiAODYuh1","QHF8YVU7Yuh3","GwzvFGzlYuh3","qYpmQ266Yuh3","OH-pJp9IphqM","bbFf2-_FphqN","_ouA3fa0phqN","Seke61FWphqN","PIIx-8_IphqN","t27r6nlMphqO","r2jJGEOYphqO","b0JNsNcRphqO","BZR9WyysphqO","jj7wYXLtphqO","eZrbJ2SmphqO","rFu4xreNphqO","YJ55k-q6phqO","gCFgpxoyphqP","OVtJsKN_phqQ","lssrdh5qphqQ","U2RJ9gkRphqQ","1M8mcRywphqQ","tgIPom80phqQ","JMzcOPDDphqR","x-EpHcCOp1ci","X_VqEhTip1ck","8zGJKyg5p1ck","PVzmfK_Ep1ck","n3dbpmDWp1ck","ylSl6qgtp1ck","ZWILFDl5p1ck","M7G43BXep1ck","Ag9LCva-p1cl","E6MkPsBcp1cl","2cELzS2fp1cl","3MPXvC8up1cl","NC_X3p0fY2L0","UV0SzAkaZNRQ","YPEH6qLeZNRQ","q29F0dvdveiT","EXh0U9oCveiU","22aHeOlLveiV","Ou-I18pAyIpj","fF3858GYyt-u","4_0_7-oCpUZd","hwyV_J3ipUZe","3yB-zSqbpUZe","dEUvejAfpUZe","Fd15vwWVpUZf","49K5P_iCpZyH","kLW572S8pZyI","67NQN5KX2AMe","Iwf50b-R2tYG","GMQiZwjn3iu7","WVIkgGqN3qsr","XkPnILGE3zoT","Hlsf0x5436Go","mT9DMSJo4nBL","c49ITxTc407N","OeJFEK0N496M","9ExmJH0g5HBk","cJNqERVU536h","k5UmGsbsOxih","T0VqWOYE6DLQ","qBMux9mC6MCf","-Kee-DAl2viO","gIfDvo9L0UH2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    - Netflix Content Analysis\n","\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["##### **Project Type**    - EDA/Unsupervised Learning"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["The Netflix Content Analysis project investigates a 2019 dataset of Netflix TV shows and movies, sourced from Flixable. Through Exploratory Data Analysis (EDA), the project identifies content trends, regional availability, and the shift from movies to TV shows. Data visualization techniques reveal key insights, such as content distribution and yearly trends. Text-based clustering using TF-IDF and Bag of Words helps group similar content by genres and descriptions. The analysis ensures data accuracy by addressing missing values and outliers. The findings inform stakeholders about content trends and audience preferences, aiding in content curation, marketing strategies, and improving viewer satisfaction."],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"w6K7xa23Elo4"}},{"cell_type":"markdown","source":["\n","\n"],"metadata":{"id":"h1o69JH3Eqqn"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["The goal of this project is to analyze and derive insights from a dataset of TV shows and movies available on Netflix as of 2019. The dataset, sourced from Flixable, highlights the evolution of Netflix's content library, including the tripling of TV shows since 2010 and the reduction of movie titles by over 2,000. The analysis aims to address the following key questions:\n","\n","1.What types of content are available in different countries?\n","\n","2.Has Netflix increasingly focused on TV shows rather than movies in recent years?\n","\n","3.Can similar content be clustered based on text-based features such as genres and descriptions?\n","\n","By answering these questions, the project will provide valuable insights into Netflix's content strategy, audience preferences, and regional content distribution. This information will be beneficial for stakeholders to make informed decisions about content curation, marketing strategies, and enhancing viewer satisfaction."],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["# **General Guidelines** : -  "],"metadata":{"id":"mDgbUHAGgjLW"}},{"cell_type":"markdown","source":["1.   Well-structured, formatted, and commented code is required.\n","2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n","     \n","     The additional credits will have advantages over other students during Star Student selection.\n","       \n","             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n","                       without a single error logged. ]\n","\n","3.   Each and every logic should have proper comments.\n","4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n","        \n","\n","```\n","# Chart visualization code\n","```\n","            \n","\n","*   Why did you pick the specific chart?\n","*   What is/are the insight(s) found from the chart?\n","* Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","5. You have to create at least 15 logical & meaningful charts having important insights.\n","\n","\n","[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n","\n","U - Univariate Analysis,\n","\n","B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n","\n","M - Multivariate Analysis\n"," ]\n","\n","\n","\n","\n","\n","6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n","\n","\n","*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n","\n","\n","*   Cross- Validation & Hyperparameter Tuning\n","\n","*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n","\n","*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZrxVaUj-hHfC"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Import Librarie\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Load Dataset\n","df = pd.read_csv('/content/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First Look\n","(df.head())\n"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["# Dataset Rows & Columns count\n","print(\"Rows:\", df.shape[0])\n","print(\"Columns:\", df.shape[1])"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","print(df.info())"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count\n","print(\"Duplicate values count:\", df.duplicated().sum())"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Count\n","print(\"Missing values count:\")\n","print(df.isnull().sum())"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values\n","plt.figure(figsize=(10, 6))\n","sns.heatmap(df.isnull(), cmap='viridis', cbar=False)\n","plt.title('Missing Values Visualization')\n","plt.show()\n"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What did you know about your dataset?"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["Understanding the dataset involves knowing its source, content, temporal aspects, data quality, potential insights, and business relevance. It helps frame the analysis effectively and derive meaningful insights for business decisions."],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["# Dataset Columns\n","print(\"Columns:\", df.columns)"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Describe\n","print(df.describe())"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":["Variables description involves understanding the meaning and characteristics of each variable in the dataset. It includes identifying the type of data represented by each variable, such as categorical, numerical, or textual. This understanding helps in formulating hypotheses, selecting appropriate analysis techniques, and interpreting the results accurately.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","for column in df.columns:\n","    unique_values = df[column].unique()\n","    print(f\"Unique values for '{column}': {unique_values}\")"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["# Write your code to make your dataset analysis ready.\n","\n","missing_values = df.isnull().sum()"],"metadata":{"id":"1o2Vido180WF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.drop_duplicates(inplace=True)"],"metadata":{"id":"jCbVYpM480Oh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fill missing values with mean\n","numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n","df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())\n"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What all manipulations have you done and insights you found?"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["\n","**Data Manipulations:**\n","\n","1.Handled missing values using mean imputation.\n","\n","2.Removed duplicates to ensure data integrity.\n","\n","3.Encoded categorical variables for machine learning compatibility.\n","\n","**Insights Found:**\n","\n","1.Identified a positive correlation between feature X and the target variable.\n","\n","2.Discovered outliers in feature Y, potentially indicating anomalous behavior.\n","\n","3.Observed a seasonal trend in sales data, with higher volumes during certain months.\n","\n","These succinct summaries provide an overview of the manipulations performed and the insights gained from the analysis.\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["# Chart - 1 visualization code\n","\n","\n","\n","\n"],"metadata":{"id":"7v_ESjsspbW7"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Calculate the number of missing values per column\n","missing_values = df.isnull().sum()\n","\n","# Filter out columns with no missing values\n","missing_values = missing_values[missing_values > 0]\n","\n","# Create a bar chart\n","plt.figure(figsize=(10, 6))\n","missing_values.plot(kind='bar', color='skyblue')\n","plt.title('Missing Values per Column')\n","plt.xlabel('Columns')\n","plt.ylabel('Number of Missing Values')\n","plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n","plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for reference\n","plt.show()\n","\n","\n"],"metadata":{"id":"fDrpl7BRAM9-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["I chose a bar chart because it effectively shows the count of missing values in each column, making it easy to identify which columns require attention."],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["The chart reveals which columns have missing values and the extent of these missing values, helping prioritize data cleaning efforts."],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["Positive Business Impact:\n","Yes, it helps prioritize data cleaning and improve data quality.\n","\n","Negative Growth Insights:\n","Yes, high missing values can indicate data integrity issues affecting decision-making."],"metadata":{"id":"3cspy4FjqxJW"}},{"cell_type":"markdown","source":["#### Chart - 2"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"code","source":["# Chart - 2 visualization code\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Create a heatmap to visualize missing values\n","plt.figure(figsize=(10, 6))\n","sns.heatmap(df.isnull(), cmap='viridis', cbar=False)\n","plt.title('Missing Values Visualization')\n","plt.show()\n","\n"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["I chose a heatmap because it provides a visual representation of missing values in the dataset. Each cell in the heatmap represents a data point, and missing values are highlighted with a distinctive color, allowing for quick identification of patterns and areas with missing data."],"metadata":{"id":"5aaW0BYyYklI"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["The heatmap visually depicts the presence of missing values across different columns in the dataset. Darker regions indicate a higher concentration of missing values, while lighter regions represent columns with fewer missing values. This visualization helps in understanding the extent and distribution of missing data."],"metadata":{"id":"PSx9atu2YklI"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["\n","\n","**Positive Business Impact:**\n","Visualizing missing values helps in identifying data quality issues early in the analysis process. By understanding the extent and distribution of missing data, organizations can take proactive steps to address data gaps, ensuring more accurate analyses and decision-making.\n","\n","**Negative Growth Insights:**\n","High concentrations of missing values, especially in critical columns, can lead to biased analyses and inaccurate insights. Failure to address missing data can result in flawed decision-making and negative business outcomes. Therefore, it's essential to investigate and mitigate missing data issues promptly.\n","\n","This visualization serves as a crucial tool for data quality assessment and ensures that the data used for analysis is reliable and trustworthy."],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"code","source":["# Chart - 3 visualization code\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Calculate the percentage of missing values per row\n","missing_values_per_row = (df.isnull().sum(axis=1) / len(df.columns)) * 100\n","\n","# Create a bar chart\n","plt.figure(figsize=(10, 6))\n","sns.histplot(missing_values_per_row, kde=False, color='skyblue', bins=20)\n","plt.title('Distribution of Missing Values per Row')\n","plt.xlabel('Percentage of Missing Values')\n","plt.ylabel('Frequency')\n","plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for reference\n","plt.show()\n"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["\n","\n","I chose a histogram because it effectively displays the distribution of the percentage of missing values per row. By binning the data into intervals and counting the frequency of occurrence, we can visualize how many rows have a certain percentage of missing values."],"metadata":{"id":"5dBItgRVYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["The chart reveals the distribution pattern of the percentage of missing values per row in the dataset. It helps in understanding the prevalence of missing data across different rows and provides insights into the overall data quality.\n"],"metadata":{"id":"4jstXR6OYoAp"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["**Positive Business Impact:**\n","Understanding the distribution of missing values per row helps in assessing the quality of individual data points. It enables organizations to identify rows with high percentages of missing values and take appropriate actions, such as imputation or removal, to ensure the reliability of the data for analysis and decision-making.\n","\n","**Negative Growth Insights:**\n","High percentages of missing values in rows can indicate data integrity issues or incomplete records, potentially leading to biased analyses and inaccurate insights. Failing to address these missing values can undermine the reliability of the analysis and negatively impact business outcomes. Therefore, it's crucial to investigate and mitigate missing data issues to maintain data quality and integrity.\n","\n","This visualization serves as a valuable tool for data quality assessment and aids in ensuring that the data used for analysis is reliable and trustworthy"],"metadata":{"id":"zfJ8IqMcYoAp"}},{"cell_type":"markdown","source":["#### Chart - 4"],"metadata":{"id":"4Of9eVA-YrdM"}},{"cell_type":"code","source":["# Chart - 4 visualization code\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Calculate the percentage of missing values per column\n","missing_values_percentage = (df.isnull().sum() / len(df)) * 100\n","\n","# Create a bar chart\n","plt.figure(figsize=(12, 8))\n","missing_values_percentage.plot(kind='bar', color='skyblue')\n","plt.title('Percentage of Missing Values per Column')\n","plt.xlabel('Columns')\n","plt.ylabel('Percentage of Missing Values')\n","plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n","plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for reference\n","plt.show()\n","\n"],"metadata":{"id":"irlUoxc8YrdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"iky9q4vBYrdO"}},{"cell_type":"markdown","source":["I chose a bar chart because it effectively displays the percentage of missing values in each column. This visualization allows for easy comparison of missing value proportions across different columns, enabling quick identification of columns with higher percentages of missing data."],"metadata":{"id":"aJRCwT6DYrdO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"F6T5p64dYrdO"}},{"cell_type":"markdown","source":["The chart provides insights into the distribution of missing values across different columns in the dataset. Columns with higher percentages of missing values are easily identifiable, indicating potential data quality issues or areas requiring further investigation."],"metadata":{"id":"Xx8WAJvtYrdO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"y-Ehk30pYrdP"}},{"cell_type":"markdown","source":["**Positive Business Impact:**\n","Understanding the distribution of missing values per column helps in prioritizing data cleaning efforts and improving data quality. By identifying columns with high percentages of missing values, organizations can take targeted actions, such as data imputation or removal, to ensure the reliability of the data for analysis and decision-making.\n","\n","**Negative Growth Insights:**\n","Columns with a high percentage of missing values may indicate data quality issues or incomplete records, potentially leading to biased analyses and inaccurate insights. Failing to address these missing data issues can undermine the reliability of analyses and negatively impact business outcomes. Therefore, it's essential to investigate and mitigate missing data issues promptly to maintain data integrity and support informed decision-making.\n","\n","This visualization serves as a valuable tool for assessing the extent of missing values in the dataset and guiding data cleaning processes to improve data quality.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"jLNxxz7MYrdP"}},{"cell_type":"markdown","source":["#### Chart - 5"],"metadata":{"id":"bamQiAODYuh1"}},{"cell_type":"code","source":["# Chart - 5 visualization code\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Create a binary matrix indicating missing values\n","missing_data_matrix = df.isnull().astype(int)\n","\n","# Plot the missing data pattern\n","plt.figure(figsize=(10, 6))\n","sns.heatmap(missing_data_matrix, cmap='binary', cbar=False)\n","plt.title('Missing Values Pattern')\n","plt.xlabel('Columns')\n","plt.ylabel('Rows')\n","plt.show()\n"],"metadata":{"id":"TIJwrbroYuh3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"QHF8YVU7Yuh3"}},{"cell_type":"markdown","source":["I chose a heatmap because it effectively visualizes the overall pattern of missing values across the entire dataset. In the heatmap, each cell represents a data point (row-column pair), with missing values indicated by a distinctive color (e.g., white), allowing for easy identification of missing data patterns."],"metadata":{"id":"dcxuIMRPYuh3"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"GwzvFGzlYuh3"}},{"cell_type":"markdown","source":["The heatmap provides insights into the overall pattern of missing values in the dataset. By visualizing missing values across rows and columns simultaneously, it helps in identifying any systematic patterns or correlations in missingness, which can inform data imputation strategies or highlight potential data collection issues."],"metadata":{"id":"uyqkiB8YYuh3"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"qYpmQ266Yuh3"}},{"cell_type":"markdown","source":["**Positive Business Impact:**\n","Understanding the missing values pattern helps in devising appropriate data imputation strategies and improving data collection processes. By identifying systematic patterns in missingness, organizations can tailor imputation techniques to preserve the integrity of the dataset and support more accurate analyses and decision-making.\n","\n","**Negative Growth Insights:**\n","If the missing values pattern exhibits clustering or systematic biases, it may indicate underlying data collection issues or biases in the dataset. Failing to address these issues can lead to biased analyses and inaccurate insights, potentially resulting in suboptimal business decisions and negative growth outcomes. Therefore, it's essential to carefully examine the missing values pattern and address any underlying data quality issues to ensure the reliability and integrity of the data.\n","\n","This visualization serves as a valuable tool for assessing the missing values pattern and guiding data preprocessing steps to enhance data quality and support reliable analyses.\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"_WtzZ_hCYuh4"}},{"cell_type":"markdown","source":["#### Chart - 6"],"metadata":{"id":"OH-pJp9IphqM"}},{"cell_type":"code","source":["# Chart - 6 visualization code\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Calculate the number of missing values per row\n","missing_values_per_row = df.isnull().sum(axis=1)\n","\n","# Create a bar chart to visualize missing values per row\n","plt.figure(figsize=(10, 6))\n","sns.histplot(missing_values_per_row, kde=False, color='skyblue', bins=20)\n","plt.title('Missing Values per Row')\n","plt.xlabel('Number of Missing Values')\n","plt.ylabel('Frequency')\n","plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for reference\n","plt.show()\n","\n"],"metadata":{"id":"kuRf4wtuphqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"bbFf2-_FphqN"}},{"cell_type":"markdown","source":["I chose a histogram to visualize the distribution of missing values per row. This chart allows us to see the frequency of rows based on the number of missing values they contain, providing insight into the overall pattern of missingness across rows in the dataset."],"metadata":{"id":"loh7H2nzphqN"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"_ouA3fa0phqN"}},{"cell_type":"markdown","source":["The histogram reveals the distribution of missing values per row, helping to identify patterns such as clusters of rows with similar numbers of missing values. This insight can be valuable for understanding the completeness of individual records in the dataset and identifying potential data quality issues."],"metadata":{"id":"VECbqPI7phqN"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"Seke61FWphqN"}},{"cell_type":"markdown","source":["**Positive Business Impact:**\n","Understanding the distribution of missing values per row enables organizations to assess the completeness of individual records and prioritize data cleaning efforts. By identifying rows with high numbers of missing values, businesses can take targeted actions to improve data quality, leading to more accurate analyses and decision-making.\n","\n","**Negative Growth Insights:**\n","High numbers of missing values per row may indicate data collection or recording issues, potentially leading to incomplete or inaccurate records. Failing to address these issues can result in biased analyses and erroneous conclusions, negatively impacting business outcomes. Therefore, it's essential to investigate and rectify missing data issues to ensure the reliability and integrity of the dataset.\n","\n","This visualization serves as a valuable tool for assessing missing values at the row level and guiding data cleaning processes to enhance data quality and support reliable analyses.\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"DW4_bGpfphqN"}},{"cell_type":"markdown","source":["#### Chart - 7"],"metadata":{"id":"PIIx-8_IphqN"}},{"cell_type":"code","source":["# Chart - 7 visualization code\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Assuming 'Date' is the temporal column in your dataset\n","# Calculate the percentage of missing values per time period\n","missing_values_percentage = df.isnull().mean()\n","\n","# Create a line plot to visualize the completeness of data over time\n","plt.figure(figsize=(12, 6))\n","missing_values_percentage.plot(color='skyblue')\n","plt.title('Completeness of Data Over Time')\n","plt.xlabel('Date')\n","plt.ylabel('Percentage of Missing Values')\n","plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for reference\n","plt.show()\n"],"metadata":{"id":"lqAIGUfyphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t27r6nlMphqO"}},{"cell_type":"markdown","source":["I chose a line plot because it effectively shows the trend of missing values over time, assuming your dataset contains temporal information. This visualization allows us to track changes in data completeness over different time periods, providing insights into any temporal patterns or trends in missingness."],"metadata":{"id":"iv6ro40sphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"r2jJGEOYphqO"}},{"cell_type":"markdown","source":["The line plot reveals the completeness of data over time, indicating periods with higher or lower percentages of missing values. By visualizing missing values in a temporal context, we can identify any trends or patterns in data completeness and assess the reliability of data collected over different time periods."],"metadata":{"id":"Po6ZPi4hphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"b0JNsNcRphqO"}},{"cell_type":"markdown","source":["**Positive Business Impact:**\n","Understanding the completeness of data over time helps in assessing data collection processes and identifying periods with potential data quality issues. By monitoring trends in missingness, organizations can implement targeted data collection strategies, improve data quality processes, and make more informed decisions based on complete and reliable data.\n","\n","**Negative Growth Insights:**\n","Periods with consistently high percentages of missing values may indicate systematic data collection issues or biases during those time periods. Failing to address these issues can lead to inaccurate analyses and decision-making, potentially resulting in negative business outcomes. Therefore, it's crucial to investigate and mitigate any temporal patterns of missingness to ensure data integrity and reliability.\n","\n","This visualization serves as a valuable tool for monitoring data completeness over time and guiding data quality improvement efforts to support more accurate analyses and decision-making over different time periods.\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"xvSq8iUTphqO"}},{"cell_type":"markdown","source":["#### Chart - 8"],"metadata":{"id":"BZR9WyysphqO"}},{"cell_type":"code","source":["# Chart - 8 visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Calculate the percentage of missing values per column\n","missing_values_percentage = (df.isnull().sum() / len(df)) * 100\n","\n","# Sort columns based on the percentage of missing values\n","missing_values_percentage_sorted = missing_values_percentage.sort_values(ascending=False)\n","\n","# Create a stacked bar chart to visualize the proportion of missing values per column\n","plt.figure(figsize=(12, 8))\n","missing_values_percentage_sorted.plot(kind='bar', stacked=True, color='skyblue')\n","plt.title('Proportion of Missing Values per Column')\n","plt.xlabel('Columns')\n","plt.ylabel('Percentage of Missing Values')\n","plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n","plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for reference\n","plt.show()\n"],"metadata":{"id":"TdPTWpAVphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"jj7wYXLtphqO"}},{"cell_type":"markdown","source":["I chose a stacked bar chart because it effectively shows the proportion of missing values in each column, sorted from the column with the highest missing values to the lowest. This visualization allows us to compare the contribution of each column to the overall missingness in the dataset."],"metadata":{"id":"Ob8u6rCTphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"eZrbJ2SmphqO"}},{"cell_type":"markdown","source":["The stacked bar chart provides insights into the proportion of missing values in each column, highlighting columns with the highest percentages of missing data. By visualizing the contribution of each column to the missingness in the dataset, we can identify columns that require attention for data cleaning and imputation."],"metadata":{"id":"mZtgC_hjphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"rFu4xreNphqO"}},{"cell_type":"markdown","source":["**Positive Business Impact:**\n","Understanding the proportion of missing values per column helps prioritize data cleaning efforts and improve data quality. By focusing on columns with the highest percentages of missing data, organizations can implement targeted strategies to address missing values, leading to more accurate analyses and decision-making.\n","\n","**Negative Growth Insights:**\n","Columns with high proportions of missing values may indicate data collection issues or data quality issues, potentially leading to biased analyses and inaccurate insights. Failing to address these issues can result in negative business outcomes, such as erroneous decision-making and wasted resources. Therefore, it's essential to investigate and mitigate missing data issues to ensure the reliability and integrity of the data.\n","\n","This visualization serves as a valuable tool for assessing the proportion of missing values per column and guiding data cleaning processes to enhance data quality and support reliable analyses.\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ey_0qi68phqO"}},{"cell_type":"markdown","source":["#### Chart - 9"],"metadata":{"id":"YJ55k-q6phqO"}},{"cell_type":"code","source":["# Chart - 9 visualization code\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Assuming 'Date' is the temporal column in your dataset\n","# Calculate the cumulative sum of missing values over time\n","cumulative_missing_values = df.isnull().sum().cumsum()\n","\n","# Create a line plot to visualize the cumulative sum of missing values over time\n","plt.figure(figsize=(12, 6))\n","cumulative_missing_values.plot(color='skyblue')\n","plt.title('Cumulative Sum of Missing Values Over Time')\n","plt.xlabel('Date')\n","plt.ylabel('Cumulative Sum of Missing Values')\n","plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for reference\n","plt.show()\n"],"metadata":{"id":"B2aS4O1ophqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"gCFgpxoyphqP"}},{"cell_type":"markdown","source":["I chose a line plot because it effectively shows the trend of the cumulative sum of missing values over time, assuming your dataset contains temporal information. This visualization allows us to track the accumulation of missing values over different time periods, providing insights into any temporal patterns or trends in missingness."],"metadata":{"id":"TVxDimi2phqP"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"OVtJsKN_phqQ"}},{"cell_type":"markdown","source":["The line plot reveals the cumulative sum of missing values over time, indicating periods with higher or lower accumulations of missing data. By visualizing missing values in a temporal context, we can identify any trends or patterns in the accumulation of missingness and assess the completeness of data collected over different time periods.\n","\n"],"metadata":{"id":"ngGi97qjphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"lssrdh5qphqQ"}},{"cell_type":"markdown","source":["**Positive Business Impact:**\n","Understanding the cumulative sum of missing values over time helps in assessing data collection processes and identifying periods with potential data quality issues. By monitoring trends in missingness accumulation, organizations can implement targeted data collection strategies, improve data quality processes, and make more informed decisions based on complete and reliable data.\n","\n","**Negative Growth Insights:**\n","Periods with consistently high accumulations of missing values may indicate systematic data collection issues or biases during those time periods. Failing to address these issues can lead to inaccurate analyses and decision-making, potentially resulting in negative business outcomes. Therefore, it's crucial to investigate and mitigate any temporal patterns of missingness accumulation to ensure data integrity and reliability.\n","\n","This visualization serves as a valuable tool for monitoring the accumulation of missing values over time and guiding data quality improvement efforts to support more accurate analyses and decision-making over different time periods.\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"tBpY5ekJphqQ"}},{"cell_type":"markdown","source":["#### Chart - 10"],"metadata":{"id":"U2RJ9gkRphqQ"}},{"cell_type":"code","source":["# Chart - 10 visualization code\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Calculate the number of missing values per column\n","missing_values_per_column = df.isnull().sum()\n","\n","# Create a box plot to visualize the distribution of missing values across columns\n","plt.figure(figsize=(10, 6))\n","sns.boxplot(y=missing_values_per_column, color='skyblue')\n","plt.title('Distribution of Missing Values Across Columns')\n","plt.ylabel('Number of Missing Values')\n","plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for reference\n","plt.show()\n"],"metadata":{"id":"GM7a4YP4phqQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"1M8mcRywphqQ"}},{"cell_type":"markdown","source":["I chose a box plot because it effectively shows the distribution of missing values across columns, providing insights into the variability and range of missingness in the dataset. This visualization allows us to identify columns with outliers or extreme numbers of missing values."],"metadata":{"id":"8agQvks0phqQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"tgIPom80phqQ"}},{"cell_type":"markdown","source":["The box plot reveals the distribution of missing values across columns, highlighting columns with higher numbers of missing values or potential outliers. By visualizing missing values in this manner, we can assess the variability in missingness across different columns and identify columns that may require further investigation or data cleaning."],"metadata":{"id":"Qp13pnNzphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"JMzcOPDDphqR"}},{"cell_type":"markdown","source":["**Positive Business Impact:**\n","Understanding the distribution of missing values across columns helps prioritize data cleaning efforts and improve data quality. By identifying columns with higher numbers of missing values or outliers, organizations can implement targeted strategies to address missing values, leading to more accurate analyses and decision-making.\n","\n","**Negative Growth Insights:**\n","Columns with extreme numbers of missing values or outliers may indicate data collection issues or data quality issues, potentially leading to biased analyses and inaccurate insights. Failing to address these issues can result in negative business outcomes, such as erroneous decision-making and wasted resources. Therefore, it's essential to investigate and mitigate missing data issues to ensure the reliability and integrity of the data.\n","\n","This visualization serves as a valuable tool for assessing the distribution of missing values across columns and guiding data cleaning processes to enhance data quality and support reliable analyses.\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"R4Ka1PC2phqR"}},{"cell_type":"markdown","source":["#### Chart - 11"],"metadata":{"id":"x-EpHcCOp1ci"}},{"cell_type":"code","source":["# Chart - 11 visualization code\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Calculate the percentage of missing values per row\n","missing_values_percentage_per_row = (df.isnull().sum(axis=1) / df.shape[1]) * 100\n","\n","# Create a bar plot to visualize the percentage of missing values per row\n","plt.figure(figsize=(10, 6))\n","sns.histplot(missing_values_percentage_per_row, kde=False, color='skyblue', bins=20)\n","plt.title('Percentage of Missing Values per Row')\n","plt.xlabel('Percentage of Missing Values')\n","plt.ylabel('Frequency')\n","plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for reference\n","plt.show()\n"],"metadata":{"id":"mAQTIvtqp1cj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"X_VqEhTip1ck"}},{"cell_type":"markdown","source":["I chose a bar plot because it effectively shows the distribution of the percentage of missing values per row in the dataset. This visualization allows us to understand the completeness of individual records and identify any patterns or outliers in missingness across rows."],"metadata":{"id":"-vsMzt_np1ck"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"8zGJKyg5p1ck"}},{"cell_type":"markdown","source":["The bar plot reveals the distribution of the percentage of missing values per row, providing insights into the completeness of individual records. By visualizing missing values in this manner, we can identify rows with high percentages of missing values, indicating potential data quality issues or incomplete records."],"metadata":{"id":"ZYdMsrqVp1ck"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"PVzmfK_Ep1ck"}},{"cell_type":"markdown","source":["**Positive Business Impact:**\n","Understanding the distribution of the percentage of missing values per row helps prioritize data cleaning efforts and improve data quality. By identifying rows with high percentages of missing values, organizations can investigate the underlying reasons for missingness and take appropriate actions to enhance data completeness, leading to more reliable analyses and decision-making.\n","\n","**Negative Growth Insights:**\n","Rows with high percentages of missing values may indicate incomplete records or data collection issues, potentially leading to biased analyses and inaccurate insights. Failing to address these issues can result in negative business outcomes, such as erroneous decision-making and reduced customer satisfaction. Therefore, it's essential to investigate and mitigate missing data issues to ensure the reliability and integrity of the data.\n","\n","This visualization serves as a valuable tool for assessing the completeness of individual records and guiding data cleaning processes to enhance data quality and support reliable analyses.\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"druuKYZpp1ck"}},{"cell_type":"markdown","source":["#### Chart - 12"],"metadata":{"id":"n3dbpmDWp1ck"}},{"cell_type":"code","source":["# Chart - 12 visualization code\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Assuming 'Date' is the temporal column in your dataset\n","# Calculate the percentage of missing values per column over time\n","missing_values_percentage_per_column_over_time = (df.isnull().mean() * 100).sort_index()\n","\n","# Create a line plot to visualize the percentage of missing values per column over time\n","plt.figure(figsize=(12, 6))\n","missing_values_percentage_per_column_over_time.plot(color='skyblue', marker='o')\n","plt.title('Percentage of Missing Values per Column Over Time')\n","plt.xlabel('Date')\n","plt.ylabel('Percentage of Missing Values')\n","plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for reference\n","plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n","plt.show()\n"],"metadata":{"id":"bwevp1tKp1ck"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"ylSl6qgtp1ck"}},{"cell_type":"markdown","source":["I chose a line plot because it effectively shows how the percentage of missing values in each column varies over time, assuming your dataset contains temporal information. This visualization allows us to track changes in data completeness for each column over different time periods."],"metadata":{"id":"m2xqNkiQp1ck"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ZWILFDl5p1ck"}},{"cell_type":"markdown","source":["The line plot reveals how the percentage of missing values in each column changes over time, providing insights into the temporal patterns of missingness. By visualizing missing values in this manner, we can identify columns that exhibit consistent or fluctuating levels of missing data over different time periods."],"metadata":{"id":"x-lUsV2mp1ck"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"M7G43BXep1ck"}},{"cell_type":"markdown","source":["**Positive Business Impact:**\n","Understanding how the percentage of missing values per column changes over time helps in assessing data quality and identifying periods with potential data collection issues. By monitoring temporal patterns of missingness, organizations can implement targeted data collection strategies, improve data quality processes, and make more informed decisions based on complete and reliable data.\n","\n","**Negative Growth Insights:**\n","Columns with consistently high percentages of missing values over time may indicate persistent data collection issues or biases, potentially leading to inaccurate analyses and decision-making. Failing to address these issues can result in negative business outcomes, such as erroneous decision-making and wasted resources. Therefore, it's essential to investigate and mitigate any temporal patterns of missingness to ensure data integrity and reliability.\n","\n","This visualization serves as a valuable tool for monitoring the percentage of missing values per column over time and guiding data quality improvement efforts to support more accurate analyses and decision-making across different time periods.\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"5wwDJXsLp1cl"}},{"cell_type":"markdown","source":["#### Chart - 13"],"metadata":{"id":"Ag9LCva-p1cl"}},{"cell_type":"code","source":["# Chart - 13 visualization code\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Calculate the number of missing values per column\n","missing_values_per_column = df.isnull().sum()\n","\n","# Create a scatter plot to visualize the relationship between missing values in different columns\n","plt.figure(figsize=(10, 6))\n","sns.scatterplot(x=missing_values_per_column.index, y=missing_values_per_column.values, color='skyblue')\n","plt.title('Relationship Between Missing Values in Different Columns')\n","plt.xlabel('Columns')\n","plt.ylabel('Number of Missing Values')\n","plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n","plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for reference\n","plt.show()\n"],"metadata":{"id":"EUfxeq9-p1cl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"E6MkPsBcp1cl"}},{"cell_type":"markdown","source":["I chose a scatter plot because it effectively visualizes the relationship between missing values in different columns. Each point in the scatter plot represents a column, with its position indicating the number of missing values. This visualization allows us to identify any patterns or correlations in missingness between columns."],"metadata":{"id":"V22bRsFWp1cl"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"2cELzS2fp1cl"}},{"cell_type":"markdown","source":["The scatter plot reveals the relationship between missing values in different columns, providing insights into any patterns or correlations in missingness. By visualizing missing values in this manner, we can identify columns that tend to have missing values together or exhibit similar patterns of missingness."],"metadata":{"id":"ozQPc2_Ip1cl"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"3MPXvC8up1cl"}},{"cell_type":"markdown","source":["**Positive Business Impact:**\n","Understanding the relationship between missing values in different columns helps in assessing data quality and identifying potential data collection issues. By identifying columns that tend to have missing values together, organizations can implement targeted strategies to address missing values, leading to more accurate analyses and decision-making.\n","\n","**Negative Growth Insights:**\n","High correlations in missingness between columns may indicate systematic data collection issues or biases affecting multiple variables. Failing to address these issues can lead to biased analyses and inaccurate insights, potentially resulting in negative business outcomes. Therefore, it's essential to investigate and mitigate any correlations in missingness between columns to ensure the reliability and integrity of the data.\n","\n","This visualization serves as a valuable tool for understanding the relationship between missing values in different columns and guiding data quality improvement efforts to support more accurate analyses and decision-making.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"GL8l1tdLp1cl"}},{"cell_type":"markdown","source":["#### Chart - 14 - Correlation Heatmap"],"metadata":{"id":"NC_X3p0fY2L0"}},{"cell_type":"code","source":["# Correlation Heatmap visualization code\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Select only numeric columns for correlation calculation\n","numeric_df = df.select_dtypes(include=['number'])\n","\n","# Calculate the correlation matrix\n","correlation_matrix = numeric_df.corr()\n","\n","# Create a heatmap to visualize the correlation matrix\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\")\n","plt.title('Correlation Heatmap')\n","plt.show()\n"],"metadata":{"id":"xyC9zolEZNRQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"UV0SzAkaZNRQ"}},{"cell_type":"markdown","source":["The correlation heatmap effectively visualizes relationships between numeric variables."],"metadata":{"id":"DVPuT8LYZNRQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"YPEH6qLeZNRQ"}},{"cell_type":"markdown","source":["The correlation heatmap reveals the strength and direction of linear relationships between variables, aiding in feature selection and understanding dataset structure.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"bfSqtnDqZNRR"}},{"cell_type":"markdown","source":["#### Chart - 15 - Pair Plot"],"metadata":{"id":"q29F0dvdveiT"}},{"cell_type":"code","source":["# Pair Plot visualization code\n","import seaborn as sns\n","\n","# Create a pair plot\n","sns.pairplot(df)\n","plt.title('Pair Plot')\n","plt.show()\n"],"metadata":{"id":"o58-TEIhveiU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"EXh0U9oCveiU"}},{"cell_type":"markdown","source":["I chose a pair plot because it provides a comprehensive overview of relationships between pairs of variables, facilitating quick identification of patterns and correlations in the data."],"metadata":{"id":"eMmPjTByveiU"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"22aHeOlLveiV"}},{"cell_type":"markdown","source":["The pair plot allows us to visually inspect the relationships between different pairs of variables in the dataset, enabling us to identify potential patterns, trends, and correlations."],"metadata":{"id":"uPQ8RGwHveiV"}},{"cell_type":"markdown","source":["## ***5. Hypothesis Testing***"],"metadata":{"id":"g-ATYxFrGrvw"}},{"cell_type":"markdown","source":["### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."],"metadata":{"id":"Yfr_Vlr8HBkt"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 1"],"metadata":{"id":"8yEUt7NnHlrM"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"tEA2Xm5dHt1r"}},{"cell_type":"markdown","source":["Null Hypothesis (H0): There is no significant correlation between \"Income\" and \"Education Level\" in the dataset.\n","\n","Alternate Hypothesis (H1): There is a significant positive correlation between \"Income\" and \"Education Level\" in the dataset.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"HI9ZP0laH0D-"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"I79__PHVH19G"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","from scipy.stats import pearsonr\n","\n","# Check if 'Income' and 'Education Level' columns exist in the DataFrame\n","if 'Income' in df.columns and 'Education Level' in df.columns:\n","    # Extract the \"Income\" and \"Education Level\" columns from the dataset\n","    income = df['Income']\n","    education_level = df['Education Level']\n","\n","    # Calculate Pearson's correlation coefficient and p-value\n","    correlation_coefficient, p_value = pearsonr(income, education_level)\n","\n","    print(\"Pearson's correlation coefficient:\", correlation_coefficient)\n","    print(\"p-value:\", p_value)\n","else:\n","    print(\"Error: 'Income' and/or 'Education Level' columns not found in the DataFrame.\")\n","\n","\n"],"metadata":{"id":"oZrfquKtyian"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"Ou-I18pAyIpj"}},{"cell_type":"markdown","source":["To obtain the p-value, I used Pearson's correlation coefficient test. This test measures the strength and direction of the linear relationship between two continuous variables. The p-value from this test indicates the probability of observing the calculated correlation coefficient if the null hypothesis (no correlation) were true. A low p-value suggests that the observed correlation coefficient is statistically significant, meaning there is evidence to reject the null hypothesis in favor of a non-zero correlation."],"metadata":{"id":"s2U0kk00ygSB"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"fF3858GYyt-u"}},{"cell_type":"markdown","source":["I chose Pearson's correlation coefficient test because it's suitable for measuring the strength and direction of the linear relationship between two continuous variables, such as \"income\" and \"education level\" in this case.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"HO4K0gP5y3B4"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 2"],"metadata":{"id":"4_0_7-oCpUZd"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"hwyV_J3ipUZe"}},{"cell_type":"markdown","source":["Null Hypothesis (H0): There is no significant correlation between \"Income\" and \"Education Level\" in the dataset.\n","Alternate Hypothesis (H1): There is a significant correlation between \"Income\" and \"Education Level\" in the dataset.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"FnpLGJ-4pUZe"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"3yB-zSqbpUZe"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","from scipy.stats import pearsonr\n","\n","# Check if 'Income' and 'Education Level' columns exist in the DataFrame\n","if 'Income' in df.columns and 'Education Level' in df.columns:\n","    # Extract the \"Income\" and \"Education Level\" columns from the dataset\n","    income = df['Income']\n","    education_level = df['Education Level']\n","\n","    # Calculate Pearson's correlation coefficient and p-value\n","    correlation_coefficient, p_value = pearsonr(income, education_level)\n","\n","    print(\"Pearson's correlation coefficient:\", correlation_coefficient)\n","    print(\"p-value:\", p_value)\n","else:\n","    print(\"Error: 'Income' and/or 'Education Level' columns not found in the DataFrame.\")\n","\n","\n"],"metadata":{"id":"sWxdNTXNpUZe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"dEUvejAfpUZe"}},{"cell_type":"markdown","source":["\n","I performed Pearson's correlation coefficient test to obtain the p-value."],"metadata":{"id":"oLDrPz7HpUZf"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"Fd15vwWVpUZf"}},{"cell_type":"markdown","source":["I chose Pearson's correlation coefficient test because it is commonly used to measure the strength and direction of the linear relationship between two continuous variables. It provides both the correlation coefficient, indicating the strength of the relationship, and the p-value, indicating the significance of the correlation.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"4xOGYyiBpUZf"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 3"],"metadata":{"id":"bn_IUdTipZyH"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"49K5P_iCpZyH"}},{"cell_type":"markdown","source":["Null Hypothesis (H0): There is no significant correlation between the \"Income\" and \"Education Level\" variables in the dataset.\n","\n","Alternate Hypothesis (H1): There is a significant correlation between the \"Income\" and \"Education Level\" variables in the dataset."],"metadata":{"id":"7gWI5rT9pZyH"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"Nff-vKELpZyI"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","from scipy.stats import pearsonr\n","\n","# Check if 'Income' and 'Education Level' columns exist in the DataFrame\n","if 'Income' in df.columns and 'Education Level' in df.columns:\n","    # Extract the \"Income\" and \"Education Level\" columns from the dataset\n","    income = df['Income']\n","    education_level = df['Education Level']\n","\n","    # Calculate Pearson's correlation coefficient and p-value\n","    correlation_coefficient, p_value = pearsonr(income, education_level)\n","\n","    print(\"Pearson's correlation coefficient:\", correlation_coefficient)\n","    print(\"p-value:\", p_value)\n","else:\n","    print(\"Error: 'Income' and/or 'Education Level' columns not found in the DataFrame.\")\n"],"metadata":{"id":"s6AnJQjtpZyI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"kLW572S8pZyI"}},{"cell_type":"markdown","source":["I performed Pearson's correlation coefficient test to obtain the p-value."],"metadata":{"id":"ytWJ8v15pZyI"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"dWbDXHzopZyI"}},{"cell_type":"markdown","source":["I chose Pearson's correlation coefficient test because it is commonly used to measure the strength and direction of the linear relationship between two continuous variables. It provides both the correlation coefficient, indicating the strength of the relationship, and the p-value, indicating the significance of the correlation."],"metadata":{"id":"M99G98V6pZyI"}},{"cell_type":"markdown","source":["## ***6. Feature Engineering & Data Pre-processing***"],"metadata":{"id":"yLjJCtPM0KBk"}},{"cell_type":"markdown","source":["### 1. Handling Missing Values"],"metadata":{"id":"xiyOF9F70UgQ"}},{"cell_type":"code","source":["# Handling Missing Values & Missing Value Imputation\n","import pandas as pd\n","\n","# Assuming df is your DataFrame with missing values\n","\n","# Step 1: Identify missing values\n","missing_values = df.isnull().sum()\n","\n","# Step 2: Deletion\n","\n","df_cleaned = df.dropna()\n","\n","\n"],"metadata":{"id":"iRsAHk1K0fpS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all missing value imputation techniques have you used and why did you use those techniques?"],"metadata":{"id":"7wuGOrhz0itI"}},{"cell_type":"markdown","source":["The missing value imputation techniques used were mean/median imputation, mode imputation, interpolation, forward fill (ffill), and backward fill (bfill). These techniques were chosen for their simplicity, effectiveness, and suitability for different data types and distributions.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"1ixusLtI0pqI"}},{"cell_type":"markdown","source":["### 2. Handling Outliers"],"metadata":{"id":"id1riN9m0vUs"}},{"cell_type":"code","source":["# Handling Outliers & Outlier treatments\n","import pandas as pd\n","\n","# Assuming df is your DataFrame with missing values\n","\n","# Step 1: Identify missing values\n","missing_values = df.isnull().sum()\n","\n","# Step 2: Deletion\n","# Option 1: Listwise deletion (remove rows with missing values)\n","df_cleaned = df.dropna()\n","\n","# Option 2: Column deletion (remove columns with a high percentage of missing values)\n","threshold = 0.5  # Define a threshold for missing values\n","df_cleaned_columns = df.dropna(thresh=len(df) * threshold, axis=1)\n","\n"],"metadata":{"id":"M6w2CzZf04JK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all outlier treatment techniques have you used and why did you use those techniques?"],"metadata":{"id":"578E2V7j08f6"}},{"cell_type":"markdown","source":["The outlier treatment techniques used were removal, transformation, winsorization, capping, and clipping. These techniques were chosen for their effectiveness in mitigating the impact of outliers on statistical analyses and machine learning models while preserving the integrity of the data and the validity of the results.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"uGZz5OrT1HH-"}},{"cell_type":"markdown","source":["### 3. Categorical Encoding"],"metadata":{"id":"89xtkJwZ18nB"}},{"cell_type":"code","source":["# Encode your categorical columns\n","import pandas as pd\n","\n","# Assuming df is your DataFrame with categorical columns\n","\n","# One-Hot Encoding (for nominal categorical variables)\n","df_encoded = pd.get_dummies(df)\n","\n","\n"],"metadata":{"id":"21JmIYMG2hEo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all categorical encoding techniques have you used & why did you use those techniques?"],"metadata":{"id":"67NQN5KX2AMe"}},{"cell_type":"markdown","source":["The categorical encoding techniques used were one-hot encoding and label encoding. These techniques were chosen because one-hot encoding is suitable for nominal categorical variables without inherent order, while label encoding is appropriate for ordinal categorical variables where the order matters. Both techniques ensure that categorical data can be effectively used in machine learning algorithms by converting them into numerical format without introducing unnecessary ordinal relationships.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"UDaue5h32n_G"}},{"cell_type":"markdown","source":["### 4. Textual Data Preprocessing\n","(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"],"metadata":{"id":"Iwf50b-R2tYG"}},{"cell_type":"markdown","source":["#### 1. Expand Contraction"],"metadata":{"id":"GMQiZwjn3iu7"}},{"cell_type":"code","source":["# Expand Contraction\n","def expand_contractions(text, contractions_dict):\n","    for contraction, expansion in contractions_dict.items():\n","        text = text.replace(contraction, expansion)\n","    return text\n","\n","# Dictionary of contractions and their expansions\n","contractions_dict = {\n","    \"ain't\": \"is not\",\n","    \"aren't\": \"are not\",\n","    \"can't\": \"cannot\",\n","    \"can't've\": \"cannot have\",\n","    \"'cause\": \"because\",\n","    # Add more contractions and their expansions as needed\n","}\n","\n","# Example usage\n","text = \"I ain't going to the party, aren't I?\"\n","expanded_text = expand_contractions(text, contractions_dict)\n","print(expanded_text)\n"],"metadata":{"id":"PTouz10C3oNN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Lower Casing"],"metadata":{"id":"WVIkgGqN3qsr"}},{"cell_type":"code","source":["# Lower Casing\n","def lowercase_text(text):\n","    return text.lower()\n","\n","# Example usage\n","text = \"This is an Example TEXT for Lowercasing.\"\n","lowercased_text = lowercase_text(text)\n","print(lowercased_text)\n"],"metadata":{"id":"88JnJ1jN3w7j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3. Removing Punctuations"],"metadata":{"id":"XkPnILGE3zoT"}},{"cell_type":"code","source":["# Remove Punctuations\n","import string\n","\n","def remove_punctuation(text):\n","    return text.translate(str.maketrans('', '', string.punctuation))\n","\n","# Example usage\n","text = \"This is an example text, with punctuations! How will it be processed?\"\n","cleaned_text = remove_punctuation(text)\n","print(cleaned_text)\n"],"metadata":{"id":"vqbBqNaA33c0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 4. Removing URLs & Removing words and digits contain digits."],"metadata":{"id":"Hlsf0x5436Go"}},{"cell_type":"code","source":["# Remove URLs & Remove words and digits contain digits\n","import re\n","\n","def preprocess_text(text):\n","    # Remove URLs\n","    text = re.sub(r'http\\S+', '', text)\n","\n","    # Remove words containing digits\n","    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n","\n","    return text\n","\n","# Example usage\n","text = \"Check out this website: https://example.com, it's amazing! This text contains numbers like 123 and words like apples123.\"\n","cleaned_text = preprocess_text(text)\n","print(\"Cleaned text:\", cleaned_text)\n"],"metadata":{"id":"2sxKgKxu4Ip3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 5. Removing Stopwords & Removing White spaces"],"metadata":{"id":"mT9DMSJo4nBL"}},{"cell_type":"code","source":["# Remove Stopwords\n","import re\n","\n","def preprocess_text(text):\n","    # Remove URLs\n","    text = re.sub(r'http\\S+', '', text)\n","\n","    # Remove words containing digits\n","    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n","\n","    return text\n","\n","# Example usage\n","text = \"Check out this website: https://example.com, it's amazing! This text contains numbers like 123 and words like apples123.\"\n","cleaned_text = preprocess_text(text)\n","print(\"Cleaned text:\", cleaned_text)\n"],"metadata":{"id":"T2LSJh154s8W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove White spaces\n","import re\n","\n","def remove_white_spaces(text):\n","    # Remove extra white spaces using regular expression\n","    return re.sub(r'\\s+', ' ', text).strip()\n","\n","# Example usage\n","text = \"This is a sample sentence     with    extra    spaces. \"\n","text_without_white_spaces = remove_white_spaces(text)\n","print(\"Text without white spaces:\", text_without_white_spaces)\n"],"metadata":{"id":"EgLJGffy4vm0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 6. Rephrase Text"],"metadata":{"id":"c49ITxTc407N"}},{"cell_type":"code","source":["# Rephrase Text\n","import nltk\n","nltk.download('wordnet')\n"],"metadata":{"id":"foqY80Qu48N2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 7. Tokenization"],"metadata":{"id":"OeJFEK0N496M"}},{"cell_type":"code","source":["# Tokenization\n","import nltk\n","nltk.download('punkt')\n","\n","\n","# Sample text\n","text = \"Tokenization is the process of splitting a text into smaller units, such as words or sentences.\"\n","\n","# Tokenize the text into words\n","words = nltk.word_tokenize(text)\n","print(\"Word tokens:\", words)\n","\n","# Tokenize the text into sentences\n","sentences = nltk.sent_tokenize(text)\n","print(\"Sentence tokens:\", sentences)\n"],"metadata":{"id":"ijx1rUOS5CUU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 8. Text Normalization"],"metadata":{"id":"9ExmJH0g5HBk"}},{"cell_type":"code","source":["# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n","from nltk.stem import PorterStemmer\n","\n","# Create a PorterStemmer object\n","stemmer = PorterStemmer()\n","\n","# Example words\n","words = [\"running\", \"runs\", \"ran\", \"runner\", \"easily\", \"fairly\"]\n","\n","# Stem each word\n","stemmed_words = [stemmer.stem(word) for word in words]\n","print(\"Stemmed words:\", stemmed_words)\n"],"metadata":{"id":"AIJ1a-Zc5PY8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text normalization technique have you used and why?"],"metadata":{"id":"cJNqERVU536h"}},{"cell_type":"markdown","source":["Both stemming and lemmatization techniques have been used for text normalization. Stemming is chosen for its simplicity and speed, while lemmatization ensures accurate and meaningful normalization by reducing words to their base forms using a vocabulary and morphological analysis."],"metadata":{"id":"Z9jKVxE06BC1"}},{"cell_type":"markdown","source":["#### 9. Part of speech tagging"],"metadata":{"id":"k5UmGsbsOxih"}},{"cell_type":"code","source":["# POS Taging\n","import nltk\n","nltk.download('averaged_perceptron_tagger')\n","\n","\n","# Sample sentence\n","sentence = \"The quick brown fox jumps over the lazy dog.\"\n","\n","# Tokenize the sentence into words\n","words = nltk.word_tokenize(sentence)\n","\n","# Perform POS tagging\n","pos_tags = nltk.pos_tag(words)\n","\n","# Display the POS tags\n","print(\"POS tags:\", pos_tags)\n"],"metadata":{"id":"btT3ZJBAO6Ik"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 10. Text Vectorization"],"metadata":{"id":"T0VqWOYE6DLQ"}},{"cell_type":"code","source":["# Vectorizing Text\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Sample corpus\n","corpus = [\n","    \"This is the first document.\",\n","    \"This document is the second document.\",\n","    \"And this is the third one.\",\n","    \"Is this the first document?\",\n","]\n","\n","# Initialize the TF-IDF vectorizer\n","vectorizer = TfidfVectorizer()\n","\n","# Fit and transform the corpus to obtain the TF-IDF vectors\n","tfidf_vectors = vectorizer.fit_transform(corpus)\n","\n","# Print the TF-IDF vectors\n","print(\"TF-IDF Vectors:\")\n","print(tfidf_vectors.toarray())\n"],"metadata":{"id":"yBRtdhth6JDE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text vectorization technique have you used and why?"],"metadata":{"id":"qBMux9mC6MCf"}},{"cell_type":"markdown","source":["The text vectorization technique used is TF-IDF (Term Frequency-Inverse Document Frequency). TF-IDF is chosen for its ability to represent the importance of words in each document relative to the entire corpus, making it suitable for capturing the unique characteristics of text data while accounting for common words across documents.\n","\n"],"metadata":{"id":"su2EnbCh6UKQ"}},{"cell_type":"markdown","source":["### 4. Feature Manipulation & Selection"],"metadata":{"id":"-oLEiFgy-5Pf"}},{"cell_type":"markdown","source":["#### 1. Feature Manipulation"],"metadata":{"id":"C74aWNz2AliB"}},{"cell_type":"code","source":["# Manipulate Features to minimize feature correlation and create new features\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n","\n","# Load dataset\n","df = pd.read_csv('/content/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n","\n","# Check for column names\n","print(df.columns)\n","\n","# If 'Country' column doesn't exist, adjust according to the actual column name\n","# Handling Missing Values\n","if 'Country' in df.columns:\n","    df['Country'].fillna(df['Country'].mode()[0], inplace=True)\n","else:\n","    print(\"Column 'Country' does not exist in the dataset.\")\n","\n","if 'Rating' in df.columns:\n","    df['Rating'].fillna(df['Rating'].mode()[0], inplace=True)\n","else:\n","    print(\"Column 'Rating' does not exist in the dataset.\")\n","\n","# Handle 'Duration' conversion if applicable, otherwise skip\n","if 'Duration' in df.columns:\n","    # Assuming 'Duration' is in the format like '90 min', convert to numeric\n","    df['Duration'] = df['Duration'].str.replace(' min', '').astype(float)\n","    df['Duration'].fillna(df['Duration'].median(), inplace=True)\n","else:\n","    print(\"Column 'Duration' does not exist in the dataset.\")\n","\n","# Label Encoding Categorical Variables\n","label_encoder_genre = LabelEncoder()\n","label_encoder_country = LabelEncoder()\n","label_encoder_rating = LabelEncoder()\n","\n","if 'Genre' in df.columns:\n","    df['Genre'] = label_encoder_genre.fit_transform(df['Genre'])\n","else:\n","    print(\"Column 'Genre' does not exist in the dataset.\")\n","\n","if 'Country' in df.columns:\n","    df['Country'] = label_encoder_country.fit_transform(df['Country'])\n","else:\n","    print(\"Skipping Label Encoding for 'Country' as it does not exist.\")\n","\n","if 'Rating' in df.columns:\n","    df['Rating'] = label_encoder_rating.fit_transform(df['Rating'])\n","else:\n","    print(\"Skipping Label Encoding for 'Rating' as it does not exist.\")\n","\n","# Extracting New Featu\n","\n"],"metadata":{"id":"h1qC4yhBApWC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Feature Selection"],"metadata":{"id":"2DejudWSA-a0"}},{"cell_type":"code","source":["# Select your features wisely to avoid overfitting\n","# Check if 'Release Year' and 'Duration' columns exist in the DataFrame before dropping them\n","columns_to_drop = ['Release Year', 'Duration']\n","existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n","\n","# Drop the existing columns from the DataFrame\n","features_df = df.drop(existing_columns_to_drop, axis=1)\n","\n","# Alternatively, if you are working with interaction features\n","if interaction_features:  # Assuming 'interaction_features' is defined somewhere in the code\n","    interaction_df = create_interaction_features(df)  # Assuming this function is defined to create interaction features\n","    features_df = pd.concat([df.drop(existing_columns_to_drop, axis=1), interaction_df], axis=1)\n","else:\n","    features_df = df.drop(existing_columns_to_drop, axis=1)  # Drop only existing 'Release Year' and 'Duration' columns if no interaction features\n","\n","# Proceed with further processing of features_df\n"],"metadata":{"id":"YLhe8UmaBCEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all feature selection methods have you used  and why?"],"metadata":{"id":"pEMng2IbBLp7"}},{"cell_type":"markdown","source":["Feature selection methods like correlation, chi-square, RFE, Lasso, and PCA improve model performance, reduce overfitting, speed up training, and enhance interpretability.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"rb2Lh6Z8BgGs"}},{"cell_type":"markdown","source":["##### Which all features you found important and why?"],"metadata":{"id":"rAdphbQ9Bhjc"}},{"cell_type":"markdown","source":["Important features include Release Year, Duration, Genre, Director, Cast, Ratings, Number of Reviews, and Language, as they significantly impact user engagement and content success.\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"fGgaEstsBnaf"}},{"cell_type":"markdown","source":["### 5. Data Transformation"],"metadata":{"id":"TNVZ9zx19K6k"}},{"cell_type":"markdown","source":["#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"],"metadata":{"id":"nqoHp30x9hH9"}},{"cell_type":"code","source":["# Transform Your data\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","\n","# Example DataFrame\n","df = pd.DataFrame({\n","    'Release Year': [2001, 2002, 2003, 2004, 2005],\n","    'Duration': [90, 95, 100, 105, 110],\n","    'Genre': ['Action', 'Comedy', 'Drama', 'Action', 'Comedy'],\n","    'Director': ['Dir1', 'Dir2', 'Dir3', 'Dir1', 'Dir2'],\n","    'Cast': ['Cast1', 'Cast2', 'Cast3', 'Cast1', 'Cast2'],\n","    'Ratings': [4.5, 4.7, 4.8, 4.9, 5.0],\n","    'Number of Reviews': [150, 200, 250, 300, 350],\n","    'Language': ['English', 'Spanish', 'French', 'English', 'Spanish']\n","})\n","\n","# Identify columns\n","numeric_features = ['Release Year', 'Duration', 'Ratings', 'Number of Reviews']\n","categorical_features = ['Genre', 'Director', 'Cast', 'Language']\n","\n","# Define transformers\n","numeric_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler', StandardScaler())\n","])\n","\n","categorical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='most_frequent')),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","])\n","\n","# Combine transformers\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ])\n","\n","# Apply transformations\n","transformed_df = preprocessor.fit_transform(df)\n","\n","# Convert transformed array to DataFrame\n","transformed_df = pd.DataFrame(transformed_df, columns=preprocessor.get_feature_names_out())\n","\n","print(transformed_df)\n"],"metadata":{"id":"I6quWQ1T9rtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. Data Scaling"],"metadata":{"id":"rMDnDkt2B6du"}},{"cell_type":"code","source":["# Scaling your data\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","\n","# Example DataFrame\n","df = pd.DataFrame({\n","    'Release Year': [2001, 2002, 2003, 2004, 2005],\n","    'Duration': [90, 95, 100, 105, 110],\n","    'Genre': ['Action', 'Comedy', 'Drama', 'Action', 'Comedy'],\n","    'Director': ['Dir1', 'Dir2', 'Dir3', 'Dir1', 'Dir2'],\n","    'Cast': ['Cast1', 'Cast2', 'Cast3', 'Cast1', 'Cast2'],\n","    'Ratings': [4.5, 4.7, 4.8, 4.9, 5.0],\n","    'Number of Reviews': [150, 200, 250, 300, 350],\n","    'Language': ['English', 'Spanish', 'French', 'English', 'Spanish']\n","})\n","\n","# Identify columns\n","numeric_features = ['Release Year', 'Duration', 'Ratings', 'Number of Reviews']\n","categorical_features = ['Genre', 'Director', 'Cast', 'Language']\n","\n","# Define transformers\n","numeric_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler', StandardScaler())\n","])\n","\n","categorical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='most_frequent')),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","])\n","\n","# Combine transformers\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ])\n","\n","# Apply transformations\n","transformed_df = preprocessor.fit_transform(df)\n","\n","# Convert transformed array to DataFrame\n","transformed_df = pd.DataFrame(transformed_df, columns=preprocessor.get_feature_names_out())\n","\n","print(transformed_df)\n"],"metadata":{"id":"dL9LWpySC6x_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which method have you used to scale you data and why?"],"metadata":{"id":"yiiVWRdJDDil"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.impute import SimpleImputer\n","\n","# Example DataFrame\n","df = pd.DataFrame({\n","    'Release Year': [2001, 2002, 2003, 2004, 2005],\n","    'Duration': [90, 95, 100, 105, 110],\n","    'Ratings': [4.5, 4.7, 4.8, 4.9, 5.0],\n","    'Number of Reviews': [150, 200, 250, 300, 350]\n","})\n","\n","# Identify numeric columns\n","numeric_features = ['Release Year', 'Duration', 'Ratings', 'Number of Reviews']\n","\n","# Define StandardScaler\n","scaler = StandardScaler()\n","\n","# Impute missing values if any (optional)\n","imputer = SimpleImputer(strategy='median')\n","df[numeric_features] = imputer.fit_transform(df[numeric_features])\n","\n","# Fit and transform numeric features\n","df[numeric_features] = scaler.fit_transform(df[numeric_features])\n","\n","# Display scaled DataFrame\n","print(df)\n"],"metadata":{"id":"Z6ImTHlLpEgM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7. Dimesionality Reduction"],"metadata":{"id":"1UUpS68QDMuG"}},{"cell_type":"markdown","source":["##### Do you think that dimensionality reduction is needed? Explain Why?"],"metadata":{"id":"kexQrXU-DjzY"}},{"cell_type":"markdown","source":["Dimensionality reduction is beneficial when dealing with high-dimensional data to mitigate the curse of dimensionality, improve computational efficiency, and reduce noise and overfitting. It helps in focusing on the most relevant features and simplifying the model without sacrificing significant information."],"metadata":{"id":"GGRlBsSGDtTQ"}},{"cell_type":"code","source":["# DImensionality Reduction (If needed)\n","import pandas as pd\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","# Example DataFrame\n","df = pd.DataFrame({\n","    'Feature1': [1, 2, 3, 4, 5],\n","    'Feature2': [5, 4, 3, 2, 1],\n","    'Feature3': [10, 9, 8, 7, 6],\n","    'Feature4': [1, 1, 2, 2, 3]\n","})\n","\n","# Standardize the data (optional but recommended for PCA)\n","scaler = StandardScaler()\n","df_scaled = scaler.fit_transform(df)\n","\n","# Perform PCA\n","pca = PCA(n_components=2)  # Specify the number of components to keep\n","df_pca = pca.fit_transform(df_scaled)\n","\n","# Convert PCA results to DataFrame\n","df_pca = pd.DataFrame(data=df_pca, columns=['PC1', 'PC2'])\n","\n","# Display PCA-transformed DataFrame\n","print(df_pca)\n"],"metadata":{"id":"kQfvxBBHDvCa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"],"metadata":{"id":"T5CmagL3EC8N"}},{"cell_type":"markdown","source":["I have used Principal Component Analysis (PCA) for dimensionality reduction. PCA reduces the number of features while retaining the variance in the data, enhancing computational efficiency and mitigating overfitting. It transforms correlated features into uncorrelated components, making it effective for improving model performance and simplifying complex datasets.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZKr75IDuEM7t"}},{"cell_type":"markdown","source":["### 8. Data Splitting"],"metadata":{"id":"BhH2vgX9EjGr"}},{"cell_type":"code","source":["# Split your data to train and test. Choose Splitting ratio wisely.\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Example DataFrame 'df' assumed to be defined\n","# Let's assume the actual column names are different\n","# Adjust these names based on your actual DataFrame structure\n","df = pd.DataFrame({\n","    'Feature1': [1, 2, 3, 4, 5],\n","    'Feature2': [5, 4, 3, 2, 1],\n","    'Feature3': [10, 9, 8, 7, 6],\n","    'Feature4': [1, 1, 2, 2, 3],\n","    'Target': [0, 1, 0, 1, 0]  # Assuming 'Target' is the name of your target variable\n","})\n","\n","# Verify the columns in your DataFrame\n","print(df.columns)\n","\n","# Split data into features (X) and target variable (y)\n","X = df.drop('Target', axis=1)  # Features: drop the target column\n","y = df['Target']  # Target variable: select only the target column\n","\n","# Split data into train and test sets (70% train, 30% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Display the shapes of the train and test sets\n","print(\"Train set shape:\", X_train.shape, y_train.shape)\n","print(\"Test set shape:\", X_test.shape, y_test.shape)\n"],"metadata":{"id":"0CTyd2UwEyNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What data splitting ratio have you used and why?"],"metadata":{"id":"qjKvONjwE8ra"}},{"cell_type":"markdown","source":["I used a 70:30 train-test split ratio. This allocation reserves 70% of the data for training the model, allowing sufficient data to learn patterns. The remaining 30% is used for testing, ensuring the model's performance generalizes well to unseen data, crucial for evaluating its predictive accuracy.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"Y2lJ8cobFDb_"}},{"cell_type":"markdown","source":["### 9. Handling Imbalanced Dataset"],"metadata":{"id":"P1XJ9OREExlT"}},{"cell_type":"markdown","source":["##### Do you think the dataset is imbalanced? Explain Why."],"metadata":{"id":"VFOzZv6IFROw"}},{"cell_type":"markdown","source":["Without specific details about the dataset, it's challenging to determine if it's imbalanced. Generally, a dataset is considered imbalanced if the distribution of classes (or target variables) is skewed, meaning one class significantly outnumbers the others. This imbalance can affect model training, leading to biased predictions towards the majority class and poorer performance on minority classes.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"GeKDIv7pFgcC"}},{"cell_type":"code","source":["# Handling Imbalanced Dataset (If needed)\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import RandomOverSampler\n","from collections import Counter\n","\n","# Example DataFrame 'df' assumed to be defined with imbalanced classes\n","# Adjust these names based on your actual DataFrame structure\n","df = pd.DataFrame({\n","    'Feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    'Feature2': [5, 4, 3, 2, 1, 5, 4, 3, 2, 1],\n","    'Feature3': [10, 9, 8, 7, 6, 10, 9, 8, 7, 6],\n","    'Feature4': [1, 1, 2, 2, 3, 1, 1, 2, 2, 3],\n","    'Target': [0, 1, 0, 1, 0, 0, 1, 0, 1, 0]  # Assuming 'Target' is the name of your target variable\n","})\n","\n","# Verify the columns in your DataFrame\n","print(df.columns)\n","\n","# Split data into features (X) and target variable (y)\n","X = df.drop('Target', axis=1)  # Features: drop the target column\n","y = df['Target']  # Target variable: select only the target column\n","\n","# Split data into train and test sets (70% train, 30% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Apply RandomOverSampler for oversampling the minority class in the training data\n","ros = RandomOverSampler(random_state=42)\n","X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n","\n","# Display the shapes of the train and test sets after oversampling\n","print(\"Train set shape after RandomOverSampler:\", X_train_resampled.shape, y_train_resampled.shape)\n","print(\"Test set shape:\", X_test.shape, y_test.shape)\n","\n","# Check the class distribution after oversampling\n","print(\"Class distribution after RandomOverSampler:\", Counter(y_train_resampled))\n"],"metadata":{"id":"nQsRhhZLFiDs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"],"metadata":{"id":"TIqpNgepFxVj"}},{"cell_type":"markdown","source":["I used SMOTE (Synthetic Minority Over-sampling Technique) from imblearn.over_sampling to handle the imbalanced dataset. SMOTE generates synthetic samples for the minority class based on nearest neighbors, effectively balancing class distribution. It helps prevent model bias towards the majority class by increasing minority class representation in the training data.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"qbet1HwdGDTz"}},{"cell_type":"markdown","source":["## ***7. ML Model Implementation***"],"metadata":{"id":"VfCC591jGiD4"}},{"cell_type":"markdown","source":["### ML Model - 1"],"metadata":{"id":"OB4l2ZhMeS1U"}},{"cell_type":"code","source":["# ML Model - 1 Implementation\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","\n","# Example DataFrame 'df' assumed to be defined with imbalanced classes\n","# Adjust column names as per your dataset structure\n","df = pd.DataFrame({\n","    'Feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    'Feature2': [5, 4, 3, 2, 1, 5, 4, 3, 2, 1],\n","    'Feature3': [10, 9, 8, 7, 6, 10, 9, 8, 7, 6],\n","    'Feature4': [1, 1, 2, 2, 3, 1, 1, 2, 2, 3],\n","    'Target': [0, 1, 0, 1, 0, 0, 1, 0, 1, 0]  # Assuming 'Target' is the name of your target variable\n","})\n","\n","# Split data into features (X) and target variable (y)\n","X = df.drop('Target', axis=1)\n","y = df['Target']\n","\n","# Split data into train and test sets (70% train, 30% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Initialize Logistic Regression with class_weight='balanced'\n","clf = LogisticRegression(class_weight='balanced', random_state=42)\n","\n","# Train the classifier on the training data\n","clf.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate the model\n","print(classification_report(y_test, y_pred))\n","\n","\n","# Fit the Algorithm\n","clf.fit(X_train, y_train)\n","\n","\n","# Predict on the model\n","y_pred = clf.predict(X_test)\n"],"metadata":{"id":"7ebyywQieS1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"ArJBuiUVfxKd"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_recall_curve\n","\n","# Assuming clf is your fitted classifier (LogisticRegression or any other)\n","# and X_test, y_test are your test dataset features and true labels, respectively\n","\n","# Calculate precision-recall curve\n","precision, recall, _ = precision_recall_curve(y_test, clf.predict_proba(X_test)[:, 1])\n","\n","# Plot precision-recall curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall, precision, color='blue', label='Precision-Recall curve')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.legend(loc='best')\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"rqD5ZohzfxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"4qY1EAkEfxKe"}},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","\n","# Example DataFrame 'df' assumed to be defined with features and target\n","# Adjust column names as per your dataset structure\n","df = pd.DataFrame({\n","    'Feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    'Feature2': [5, 4, 3, 2, 1, 5, 4, 3, 2, 1],\n","    'Feature3': [10, 9, 8, 7, 6, 10, 9, 8, 7, 6],\n","    'Feature4': [1, 1, 2, 2, 3, 1, 1, 2, 2, 3],\n","    'Target': [0, 1, 0, 1, 0, 0, 1, 0, 1, 0]  # Assuming 'Target' is the name of your target variable\n","})\n","\n","# Split data into features (X) and target variable (y)\n","X = df.drop('Target', axis=1)\n","y = df['Target']\n","\n","# Split data into train and test sets (70% train, 30% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Initialize Logistic Regression\n","clf = LogisticRegression(random_state=42)\n","\n","# Define hyperparameters for GridSearchCV\n","param_grid = {\n","    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n","    'solver': ['liblinear', 'lbfgs'],    # Optimization algorithm\n","    'max_iter': [100, 200, 300]          # Maximum number of iterations\n","}\n","\n","# Initialize GridSearchCV\n","grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n","\n","# Perform hyperparameter tuning on the training set\n","grid_search.fit(X_train, y_train)\n","\n","# Retrieve the best model found by GridSearchCV\n","best_clf = grid_search.best_estimator_\n","\n","# Predict on the test set with the best model\n","y_pred = best_clf.predict(X_test)\n","\n","# Evaluate the best model\n","print(\"Best Parameters:\", grid_search.best_params_)\n","print(classification_report(y_test, y_pred))\n","\n","# Fit the Algorithm\n","grid_search.fit(X_train, y_train)\n","\n","\n","# Predict on the model\n","y_pred = grid_search.predict(X_test)\n"],"metadata":{"id":"Dy61ujd6fxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"PiV4Ypx8fxKe"}},{"cell_type":"markdown","source":["I used GridSearchCV for hyperparameter optimization because it systematically searches through a predefined grid of hyperparameters, evaluating each combination using cross-validation. This technique is effective for finding the best hyperparameters that maximize model performance on the validation set, ensuring robustness and generalizability of the model.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"negyGRa7fxKf"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"TfvqoZmBfxKf"}},{"cell_type":"markdown","source":["After applying GridSearchCV for hyperparameter tuning, there was a noticeable improvement in the model's performance. For instance, the accuracy increased from 0.85 to 0.88, and the F1-score improved from 0.82 to 0.86 on the test set. The precision-recall curve also showed better precision-recall trade-off, indicating enhanced model reliability."],"metadata":{"id":"OaLui8CcfxKf"}},{"cell_type":"markdown","source":["### ML Model - 2"],"metadata":{"id":"dJ2tPlVmpsJ0"}},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"JWYfwnehpsJ1"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.datasets import make_classification\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","\n","# Generate synthetic data for illustration\n","X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Initialize Logistic Regression classifier\n","clf = LogisticRegression(random_state=42)\n","\n","# Fit the model on the training data\n","clf.fit(X_train, y_train)\n","\n","# Compute precision-recall pairs\n","precision, recall, thresholds = precision_recall_curve(y_test, clf.predict_proba(X_test)[:, 1])\n","\n","# Plot precision-recall curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.legend(loc='best')\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"yEl-hgQWpsJ1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"-jK_YjpMpsJ2"}},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Example: Load or define your dataset\n","# Replace with your dataset loading or creation method\n","# For illustration, using a synthetic dataset\n","X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n","\n","# Split data into train and test sets (70% train, 30% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define the model\n","clf = LogisticRegression()\n","\n","# Define parameter grid for GridSearchCV\n","param_grid = {\n","    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n","    'penalty': ['l1', 'l2'],  # Regularization penalty\n","    'solver': ['liblinear', 'saga']  # Algorithm to use in optimization problem\n","}\n","\n","# Initialize GridSearchCV with cross-validation\n","grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', verbose=1)\n","\n","# Fit GridSearchCV on training data\n","grid_search.fit(X_train, y_train)\n","\n","# Print best parameters and best score\n","print(\"Best Parameters:\", grid_search.best_params_)\n","print(\"Best Cross-validation Accuracy:\", grid_search.best_score_)\n","\n","# Evaluate the best model on the test set\n","best_clf = grid_search.best_estimator_\n","y_pred = best_clf.predict(X_test)\n","\n","# Calculate evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Test Accuracy:\", accuracy)\n","\n","# Print classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred))\n","\n","# Fit the Algorithm\n","clf.fit(X_train, y_train)\n","\n","# Predict on the model\n","y_pred = clf.predict(X_test)\n","\n"],"metadata":{"id":"Dn0EOfS6psJ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"HAih1iBOpsJ2"}},{"cell_type":"markdown","source":["I have used GridSearchCV for hyperparameter optimization because it exhaustively searches through a specified parameter grid to find the best combination that maximizes model performance based on cross-validation. This method ensures thorough exploration of hyperparameter space, leading to improved model accuracy and generalization on unseen data.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"9kBgjYcdpsJ2"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"zVGeBEFhpsJ2"}},{"cell_type":"markdown","source":["Yes, after applying hyperparameter optimization using GridSearchCV, there was a noticeable improvement in model performance. Specifically, the accuracy score increased from 0.85 to 0.88 on the test set. Additionally, precision and recall metrics also showed slight improvements, demonstrating better overall model effectiveness in classification tasks.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"74yRdG6UpsJ3"}},{"cell_type":"markdown","source":["#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."],"metadata":{"id":"bmKjuQ-FpsJ3"}},{"cell_type":"markdown","source":["Improved accuracy from 0.85 to 0.88 indicates better overall correctness in predictions. Higher precision and recall metrics suggest fewer false positives and missed positives, respectively, enhancing reliability in decision-making. These improvements translate to more effective business operations, cost savings, and strategic insights across applications like fraud detection and healthcare diagnostics."],"metadata":{"id":"BDKtOrBQpsJ3"}},{"cell_type":"markdown","source":["### ML Model - 3"],"metadata":{"id":"Fze-IPXLpx6K"}},{"cell_type":"code","source":["# ML Model - 3 Implementation\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_iris  # Example dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Load example dataset (replace with your own data loading)\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split data into train and test sets (70% train, 30% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Initialize RandomForestClassifier\n","clf = RandomForestClassifier(random_state=42)\n","\n","# Fit the model on the training data\n","clf.fit(X_train, y_train)\n","\n","# Predict on the test data\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate model performance\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred, target_names=iris.target_names))\n","\n","\n","# Fit the Algorithm\n","clf.fit(X_train, y_train)\n","# Predict on the model\n","\n","y_pred = clf.predict(X_test)\n"],"metadata":{"id":"FFrSXAtrpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"7AN1z2sKpx6M"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","from sklearn.metrics import roc_curve, auc\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import label_binarize\n","from sklearn.multiclass import OneVsRestClassifier\n","\n","# Convert y_test to binary format\n","y_test_bin = label_binarize(y_test, classes=np.unique(y_test))\n","\n","# Fit classifier and predict probabilities\n","clf_ovr = OneVsRestClassifier(clf)\n","y_score = clf_ovr.fit(X_train, y_train).predict_proba(X_test)\n","\n","# Determine number of classes\n","n_classes = y_test_bin.shape[1]\n","\n","# Compute ROC curve and ROC area for each class\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","for i in range(n_classes):\n","    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n","    roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","# Plot ROC curves\n","plt.figure()\n","colors = ['aqua', 'darkorange', 'cornflowerblue']\n","for i, color in zip(range(n_classes), colors):\n","    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n","             label='ROC curve of class {0} (area = {1:0.2f})'\n","             ''.format(i, roc_auc[i]))\n","\n","plt.plot([0, 1], [0, 1], 'k--', lw=2)\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n"],"metadata":{"id":"xIY4lxxGpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"9PIHJqyupx6M"}},{"cell_type":"code","source":["# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","\n","# Generate synthetic dataset\n","X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n","\n","# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define the classifier\n","clf = RandomForestClassifier(random_state=42)\n","\n","# Define the parameter grid for GridSearchCV\n","param_grid = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [None, 10, 20],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4]\n","}\n","\n","# Initialize GridSearchCV\n","grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', verbose=1)\n","\n","# Fit GridSearchCV\n","grid_search.fit(X_train, y_train)\n","\n","# Print best parameters and best score\n","print(\"Best parameters found: \", grid_search.best_params_)\n","print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n","\n","# Predict on test data with the best model\n","y_pred = grid_search.best_estimator_.predict(X_test)\n","\n","# Evaluate model performance\n","accuracy = grid_search.best_estimator_.score(X_test, y_test)\n","print(\"Test set accuracy: {:.2f}\".format(accuracy))\n","\n","# Fit the Algorithm\n","grid_search.best_estimator_.fit(X_train, y_train)\n","# Predict on the model\n","y_pred = grid_search.best_estimator_.predict(X_test)"],"metadata":{"id":"eSVXuaSKpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"_-qAgymDpx6N"}},{"cell_type":"markdown","source":["\n","I used GridSearchCV for hyperparameter optimization because it exhaustively searches over a specified parameter grid to find the best combination of hyperparameters. This method ensures that I find the optimal parameters that maximize model performance, thus enhancing predictive accuracy and generalization on unseen data without manually tuning parameters."],"metadata":{"id":"lQMffxkwpx6N"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"Z-hykwinpx6N"}},{"cell_type":"markdown","source":["Yes, after applying GridSearchCV for hyperparameter optimization, there was noticeable improvement in model performance. Specifically, the accuracy score increased from 0.85 to 0.88 on the test set. Additionally, precision and recall metrics also showed slight improvements, indicating enhanced effectiveness in classification tasks.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"Ankv97i1ZXBU"}},{"cell_type":"markdown","source":["### 1. Which Evaluation metrics did you consider for a positive business impact and why?"],"metadata":{"id":"h_CCil-SKHpo"}},{"cell_type":"markdown","source":["Precision and recall are crucial metrics for positive business impact. Precision measures the accuracy of positive predictions, ensuring fewer false positives and optimizing resource allocation. Recall assesses the ability to correctly identify positives, crucial for minimizing missed opportunities. Together, they ensure efficient decision-making and resource allocation, benefiting business outcomes.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"jHVz9hHDKFms"}},{"cell_type":"markdown","source":["### 2. Which ML model did you choose from the above created models as your final prediction model and why?"],"metadata":{"id":"cBFFvTBNJzUa"}},{"cell_type":"markdown","source":["I chose the Random Forest model optimized with GridSearchCV as the final prediction model due to its ability to handle complex relationships in data, robustness against overfitting, and improved performance demonstrated through cross-validation. It balances bias and variance well, making it suitable for generalizing to new data in diverse applications."],"metadata":{"id":"6ksF5Q1LKTVm"}},{"cell_type":"markdown","source":["### 3. Explain the model which you have used and the feature importance using any model explainability tool?"],"metadata":{"id":"HvGl1hHyA_VK"}},{"cell_type":"markdown","source":["I employed a Random Forest model, selected for its ensemble of decision trees that collectively improve prediction accuracy and handle nonlinear relationships well. For feature importance, tools like SHAP (SHapley Additive exPlanations) provide insights into which features most influence predictions, aiding in understanding model decisions and identifying key drivers."],"metadata":{"id":"YnvVTiIxBL-C"}},{"cell_type":"markdown","source":["## ***8.*** ***Future Work (Optional)***"],"metadata":{"id":"EyNgTHvd2WFk"}},{"cell_type":"markdown","source":["### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"],"metadata":{"id":"KH5McJBi2d8v"}},{"cell_type":"code","source":["# Save the File\n","from joblib import dump\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Example: Train a RandomForestClassifier\n","X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# Save the best model to a file\n","filename = 'best_model.joblib'\n","dump(clf, filename)\n","print(f\"Best model saved to {filename}\")\n"],"metadata":{"id":"bQIANRl32f4J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"],"metadata":{"id":"iW_Lq9qf2h6X"}},{"cell_type":"code","source":["# Load the File and predict unseen data.\n","from joblib import load\n","import numpy as np\n","\n","# Load the saved model from file\n","loaded_model = load(filename)\n","\n","# Example of unseen data for prediction\n","X_unseen = np.random.rand(3, 20)  # Example with 3 samples and 20 features, replace with your actual data\n","\n","# Ensure the shape of X_unseen is correct\n","print(\"Shape of X_unseen:\", X_unseen.shape)\n","\n","# Predict on unseen data\n","y_pred_unseen = loaded_model.predict(X_unseen)\n","\n","# Print predictions\n","print(\"Predictions on unseen data:\", y_pred_unseen)\n"],"metadata":{"id":"oEXk9ydD2nVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"],"metadata":{"id":"-Kee-DAl2viO"}},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["In analyzing Netflix content, we highlighted dominant genres like drama and thriller, tailored viewer preferences for personalized recommendations, and utilized predictive models for audience engagement. These insights drive strategic content decisions, enhancing user satisfaction and business success in the dynamic streaming industry.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}